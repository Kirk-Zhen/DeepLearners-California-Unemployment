{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMpvnaJBb3y2zdKAG5W4ARO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["**Load the data from drive**"],"metadata":{"id":"4CgWRHhR6N3g"}},{"cell_type":"code","source":["#Load and print dataframe\n","import pandas as pd\n","from google.colab import drive\n","import tensorflow as tf\n","import copy as copy\n","\n","drive.mount('/content/drive')\n","path = '/content/drive/MyDrive/IE534 itn/Project/Data'\n","df=pd.read_pickle(path+'/rnn_dataframe.pkl')\n","#Unemployment= [4]\n","df.head(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"As1WWyMaQrW2","executionInfo":{"status":"ok","timestamp":1668912798808,"user_tz":360,"elapsed":2694,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"3a3485c8-70e8-4a3f-f879-6f68d07efeec"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["                            area_name  \\\n","0                          California   \n","1                  Los Angeles County   \n","2  Los Angeles-Long Beach-Glendale MD   \n","3                      Alameda County   \n","4                       Alpine County   \n","\n","                                         labor_force  \\\n","0  [9672362, 9684440, 9689626, 9692493, 9712533, ...   \n","1  [3364151, 3364401, 3361820, 3362757, 3349943, ...   \n","2  [3364151, 3364401, 3361820, 3362757, 3349943, ...   \n","3  [676500, 673100, 672900, 670500, 671500, 67700...   \n","4  [600, 640, 620, 470, 270, 250, 220, 220, 230, ...   \n","\n","                                          employment  \\\n","0  [8668016, 8704564, 8776344, 8835232, 8945196, ...   \n","1  [3040058, 3050306, 3069797, 3082562, 3089152, ...   \n","2  [3040058, 3050306, 3069797, 3082562, 3089152, ...   \n","3  [650200, 648100, 648700, 645100, 646300, 64990...   \n","4  [580, 620, 600, 450, 220, 210, 190, 200, 180, ...   \n","\n","                                        unemployment  \\\n","0  [1004346, 979876, 913282, 857261, 767337, 8924...   \n","1  [324093, 314095, 292023, 280195, 260791, 29694...   \n","2  [324093, 314095, 292023, 280195, 260791, 29694...   \n","3  [26300, 25000, 24200, 25400, 25200, 27100, 306...   \n","4  [20, 20, 20, 20, 50, 40, 40, 20, 50, 50, 50, 6...   \n","\n","                                   unemployment_rate  \n","0  [0.104, 0.101, 0.094, 0.088, 0.079, 0.09, 0.09...  \n","1  [0.096, 0.093, 0.087, 0.083, 0.078, 0.088, 0.0...  \n","2  [0.096, 0.093, 0.087, 0.083, 0.078, 0.088, 0.0...  \n","3  [0.039, 0.037, 0.036, 0.038, 0.037, 0.04, 0.04...  \n","4  [0.025, 0.033, 0.034, 0.051, 0.169, 0.159, 0.1...  "],"text/html":["\n","  <div id=\"df-1f32972b-a2d2-4f08-9184-0f8e1e08b195\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>area_name</th>\n","      <th>labor_force</th>\n","      <th>employment</th>\n","      <th>unemployment</th>\n","      <th>unemployment_rate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>California</td>\n","      <td>[9672362, 9684440, 9689626, 9692493, 9712533, ...</td>\n","      <td>[8668016, 8704564, 8776344, 8835232, 8945196, ...</td>\n","      <td>[1004346, 979876, 913282, 857261, 767337, 8924...</td>\n","      <td>[0.104, 0.101, 0.094, 0.088, 0.079, 0.09, 0.09...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Los Angeles County</td>\n","      <td>[3364151, 3364401, 3361820, 3362757, 3349943, ...</td>\n","      <td>[3040058, 3050306, 3069797, 3082562, 3089152, ...</td>\n","      <td>[324093, 314095, 292023, 280195, 260791, 29694...</td>\n","      <td>[0.096, 0.093, 0.087, 0.083, 0.078, 0.088, 0.0...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Los Angeles-Long Beach-Glendale MD</td>\n","      <td>[3364151, 3364401, 3361820, 3362757, 3349943, ...</td>\n","      <td>[3040058, 3050306, 3069797, 3082562, 3089152, ...</td>\n","      <td>[324093, 314095, 292023, 280195, 260791, 29694...</td>\n","      <td>[0.096, 0.093, 0.087, 0.083, 0.078, 0.088, 0.0...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Alameda County</td>\n","      <td>[676500, 673100, 672900, 670500, 671500, 67700...</td>\n","      <td>[650200, 648100, 648700, 645100, 646300, 64990...</td>\n","      <td>[26300, 25000, 24200, 25400, 25200, 27100, 306...</td>\n","      <td>[0.039, 0.037, 0.036, 0.038, 0.037, 0.04, 0.04...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Alpine County</td>\n","      <td>[600, 640, 620, 470, 270, 250, 220, 220, 230, ...</td>\n","      <td>[580, 620, 600, 450, 220, 210, 190, 200, 180, ...</td>\n","      <td>[20, 20, 20, 20, 50, 40, 40, 20, 50, 50, 50, 6...</td>\n","      <td>[0.025, 0.033, 0.034, 0.051, 0.169, 0.159, 0.1...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f32972b-a2d2-4f08-9184-0f8e1e08b195')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1f32972b-a2d2-4f08-9184-0f8e1e08b195 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1f32972b-a2d2-4f08-9184-0f8e1e08b195');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["**Extract unemployment rate and total workforce as features**"],"metadata":{"id":"Fa9G0MVQ6cNp"}},{"cell_type":"code","source":["#lets use the first index which has 561 observation as training\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","\n","\n","x_total=np.asarray([df.iloc[0][4],df.iloc[0][1]])\n","# decomposed_data=seasonal_decompose(x_total, model='additive',period=1)\n","# decomposed_data.plot()"],"metadata":{"id":"YJzrOW6URwwY","executionInfo":{"status":"ok","timestamp":1668912798811,"user_tz":360,"elapsed":28,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["**Partition data into train and test and validation**\n","\n","The first 80% of data is training data, the next 10% of data is validation data and the last 10% of data is test data"],"metadata":{"id":"lfPz4Fgh7iN6"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","\n","#We use the split twice...\n","x_total_normalized=scaler.fit_transform(x_total.reshape(-1, 2))\n","train, test_and_val = train_test_split(x_total_normalized, test_size=0.2, shuffle=False)\n","\n","validation, test = train_test_split(test_and_val, test_size=0.5, shuffle=False)\n","\n","print(\"Train shape: \", train.shape)\n","print(\"Validation shape: \", validation.shape)\n","print(\"Test shape: \",test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YbM7Jf6BUldI","executionInfo":{"status":"ok","timestamp":1668912798812,"user_tz":360,"elapsed":26,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"acbe535c-efa2-4083-e517-b838a5d110c5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape:  (448, 2)\n","Validation shape:  (56, 2)\n","Test shape:  (57, 2)\n"]}]},{"cell_type":"markdown","source":["**Define number of features and length of look back window and timeseries data generator**"],"metadata":{"id":"0eV9tzqh7-6G"}},{"cell_type":"code","source":["from scipy.signal.windows import general_cosine\n","from keras.preprocessing.sequence import TimeseriesGenerator\n","\n","#look_back.We create windows of this length and try to predict the next value\n","#And we will move the window one by one\n","#Play with this!\n","n_input = 6\n","\n","#if multivariate, it will be more than 1\n","n_features = 2\n","\n","generatorTrain = TimeseriesGenerator(train, train, length=n_input, batch_size=1)\n","generatorTest= TimeseriesGenerator(test, test, length=n_input, batch_size=1)\n","generatorValidation= TimeseriesGenerator(validation, validation, length=n_input, batch_size=1)"],"metadata":{"id":"RKSLRCQBWDem","executionInfo":{"status":"ok","timestamp":1668912798816,"user_tz":360,"elapsed":27,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#lets see what is in a single batch window\n","batch_0 = generatorTrain[0]\n","x, y= batch_0\n","\n","print(x,y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVuZutHIYOMG","executionInfo":{"status":"ok","timestamp":1668912798819,"user_tz":360,"elapsed":29,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"16df02cb-6ff8-415b-a96a-75f4a5c51bd4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[3.50207587e-09 3.21941050e-09]\n","  [2.99452865e-09 2.55508770e-09]\n","  [2.23320780e-09 2.65729120e-09]\n","  [3.04528337e-09 2.70839296e-09]\n","  [2.69000031e-09 2.55508770e-09]\n","  [2.94377392e-09 2.65729120e-09]]] [[3.24830226e-09 3.11720699e-09]]\n"]}]},{"cell_type":"markdown","source":["**Build a deep learning model for the dataset. We used LSTM model here**\n","\n","LSTM networks combat the RNN's vanishing gradients or long-term dependence issue. Gradient vanishing refers to the loss of information in a neural netw\n","ork as connections recur over a longer period. The model is defined below. Since we have 2 features, we did not stack multiple LSTM layers"],"metadata":{"id":"q0XPkPsb-ViH"}},{"cell_type":"code","source":["#Vanilla LSTM!\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM, Dropout\n","\n","\n","#Create the network and define the model\n","\n","model = Sequential()\n","#300 is the shape of output\n","# model.add(LSTM(300, activation='tanh', input_shape=(n_input, n_features)))\n","model.add(LSTM(300, activation='tanh', input_shape=(n_input, n_features)))\n","model.add(Dense(1))"],"metadata":{"id":"vHTa-srhYkSx","executionInfo":{"status":"ok","timestamp":1668921493673,"user_tz":360,"elapsed":305,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["**Investigate effects of mini-batch learning**\n","\n","In this section we use several batch sizes. Sevral powers of two batch_size will be tried and the loss on validation data is analyzed. The same model with same initialization would be trained for the same number of epochs for fair comparision. Based on these experiments the **Optimal batch size is 64** according to dev set. We also used early stopping to retain the best obtained model."],"metadata":{"id":"-TiBFDCcTLin"}},{"cell_type":"code","source":["import absl.logging\n","from tqdm import tqdm\n","\n","absl.logging.set_verbosity(absl.logging.ERROR)\n","\n","batch_sizes=[1, 2, 4, 8, 16, 32, 64, 128]\n","EPOCHS=30\n","\n","model.compile(optimizer='adam', loss='mse')\n","callback= tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", min_delta=0, patience=10, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True,)\n","validation_loss=[]\n","\n","for i in tqdm(range(len(batch_sizes))):\n","  batch_size=batch_sizes[i]\n","  #clone model for fair comparision. Rather than continuing training, just train the model from scratch\n","  cloned_model=copy.deepcopy(model)\n","  cloned_model.fit(generatorTrain, epochs=EPOCHS, batch_size=batch_size, shuffle=False, validation_data=generatorValidation, verbose=0, callbacks=[callback])\n","  loss=cloned_model.evaluate(generatorValidation)\n","  validation_loss.append(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_6MawOoXJ3V","executionInfo":{"status":"ok","timestamp":1668913237246,"user_tz":360,"elapsed":435212,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"7a933b4f-9147-41e5-fc84-052957ed8725"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/8 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 1.1620e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▎        | 1/8 [00:44<05:13, 44.83s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 1.3633e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 2/8 [01:30<04:31, 45.23s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 1.3617e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 3/8 [02:34<04:29, 53.91s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 1.0543e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 4/8 [03:39<03:53, 58.25s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 2.5262e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▎   | 5/8 [04:23<02:39, 53.15s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 2.7056e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▌  | 6/8 [05:29<01:55, 57.61s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 8.7404e-05\n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 7/8 [06:25<00:57, 57.03s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 1.3193e-04\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [07:15<00:00, 54.39s/it]\n"]}]},{"cell_type":"code","source":["from matplotlib.pyplot import figure\n","\n","figure(figsize=(20, 15), dpi=80)\n","plt.subplot(211)\n","plt.plot(np.asarray(batch_sizes), np.asarray(validation_loss), 'b')\n","plt.scatter(np.asarray(batch_sizes), np.asarray(validation_loss), c='red')\n","\n","plt.xlabel(\"Batch size\")\n","plt.ylabel(\"Loss\")\n","plt.legend(['MSE loss'], loc='upper right')\n","plt.title(\"Batch size effect\")\n","\n","optimal_batch_size= batch_sizes[np.argmin(np.asarray(validation_loss))]\n","\n","print(\"Oprimal batch size according to validation data is \", optimal_batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":512},"id":"JKTBlizEZq5M","executionInfo":{"status":"ok","timestamp":1668913238969,"user_tz":360,"elapsed":1729,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"2d6b9524-12bc-42d7-f043-23acb89f63ab"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Oprimal batch size according to validation data is  64\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1600x1200 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["**Comment on size of minibatch**\n","\n","When training a model, a batch  of data is a fixed number of training data is fed to the model and the model undergoes one training step which is an update for its parameters with respect to the gradients of the loss of the batch.\n","\n","There is a myth stating that batch size should be power of 2 since it can be feed into an entire GPU for an efficient and fast training. But it is not the [case](https://wandb.ai/datenzauberai/Batch-Size-Testing/reports/Do-Batch-Sizes-Actually-Need-to-be-Powers-of-2---VmlldzoyMDkwNDQx) and although power of 2 is conventional, it is not the only option.\n","\n","Large batch sizes boosts the training speed, but will lead to sharp minima and will suffer from low generalization and lower test performance. Moreover, care should be taken than all the samples in the batch fit in the GPU memory, since model is cloned once for each training sample in the batch.\n","\n","In contrast, smaller bath sizes lead to flatter minima that generalizes bettr to the test set. The phenomena happens since smaller batches are more likely to contain more diverse sample which introduces more sthochasticity into the model that causes better regularization.\n","\n","Finally, it should be noted that no unique batch size is good and it is totally dependant on the model architecture and the target dataset and it should be tuned on the dev set. It should be noted that the learning rate should also be adjusted for batch size. Bigger batch sizes should be accompanied by bigger learning rate and vice versa, since it is desired to make changes based on the size of observed samples."],"metadata":{"id":"CSPl5jm8lIUm"}},{"cell_type":"markdown","source":["**Effect of different optimizers**\n","\n","Various Keras optimizers are used and their effect is evaluated on the validation set. We used early stopping and retain the best reached model achieved so far during training. Each optimizer is used on a cloned version of a model with equal initilization and for same number of epochs."],"metadata":{"id":"6P4RNmbKdC7p"}},{"cell_type":"code","source":["optimizers_name= ['RMSprop', 'Adam', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl', 'SGD']\n","EPOCHS=30\n","\n","\n","callback= tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", min_delta=0, patience=10, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True,)\n","validation_loss=[]\n","\n","for i in tqdm(range(len(optimizers_name))):\n","  optimizer=optimizers_name[i]\n","  #clone model for fair comparision. Rather than continuing training, just train the model from scratch\n","  cloned_model=copy.deepcopy(model)\n","  cloned_model.compile(optimizer=optimizer, loss='mse')\n","  cloned_model.fit(generatorTrain, epochs=EPOCHS, batch_size=optimal_batch_size, shuffle=False, validation_data=generatorValidation, verbose=0, callbacks=[callback])\n","  loss=cloned_model.evaluate(generatorValidation)\n","  validation_loss.append(loss)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2TQrqqEwdqJA","executionInfo":{"status":"ok","timestamp":1668913837901,"user_tz":360,"elapsed":598958,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"6d547b5f-a241-4ee3-995e-5853d02e406d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/8 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 1.1642e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▎        | 1/8 [00:51<06:01, 51.60s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 2.1302e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 2/8 [02:02<06:16, 62.69s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 0.2455\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 3/8 [03:22<05:54, 70.96s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 0.0030\n"]},{"output_type":"stream","name":"stderr","text":["\r 50%|█████     | 4/8 [04:38<04:51, 72.83s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 4.2525e-05\n"]},{"output_type":"stream","name":"stderr","text":["\r 62%|██████▎   | 5/8 [05:37<03:24, 68.00s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 8.7915e-05\n"]},{"output_type":"stream","name":"stderr","text":["\r 75%|███████▌  | 6/8 [06:40<02:12, 66.30s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 0.6497\n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 7/8 [08:25<01:18, 78.85s/it]"]},{"output_type":"stream","name":"stdout","text":["50/50 [==============================] - 0s 3ms/step - loss: 9.8457e-04\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 8/8 [09:59<00:00, 74.90s/it]\n"]}]},{"cell_type":"code","source":["for i in range(len(optimizers_name)):\n","  print(\"Optimizer \", optimizers_name[i], \" achieved validation loss of \", validation_loss[i])\n","\n","optimal_optimizer= optimizers_name[np.argmin(np.asarray(validation_loss))]\n","\n","print(\"Oprimal Optimizer according to validation data is \", optimal_optimizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7mgYsAhOhgPu","executionInfo":{"status":"ok","timestamp":1668913837901,"user_tz":360,"elapsed":12,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"a880b2f2-5bf8-4241-dad5-e2392df7eaad"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimizer  RMSprop  achieved validation loss of  0.00011641762830549851\n","Optimizer  Adam  achieved validation loss of  0.00021302240202203393\n","Optimizer  Adadelta  achieved validation loss of  0.2454807609319687\n","Optimizer  Adagrad  achieved validation loss of  0.003006086451932788\n","Optimizer  Adamax  achieved validation loss of  4.252516373526305e-05\n","Optimizer  Nadam  achieved validation loss of  8.791458822088316e-05\n","Optimizer  Ftrl  achieved validation loss of  0.6497360467910767\n","Optimizer  SGD  achieved validation loss of  0.0009845702443271875\n","Oprimal Optimizer according to validation data is  Adamax\n"]}]},{"cell_type":"markdown","source":["****Comment on the effect of optimizer****\n","\n","Optimizers are the strategies to drive the model to the optimial parameters based on the training data. They can be devided into two general catagoriez: \n","\n","1- Gradient decent optimizers which have fixed learning rate. They are either batch, stochastic or mini batch \n","\n","2- Adaptive optimizers which do not require learning rate tuning and learning rate is automatically adjusted. They are Adagrad, Adadelta, RMSprop, Adam\n","\n","A short description from each algorithm is brought from [1](https://towardsdatascience.com/7-tips-to-choose-the-best-optimizer-47bb9c1219e), [2](https://machinelearningmastery.com/gradient-descent-optimization-with-adamax-from-scratch/), [3](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/Ftrl#:~:text=%22Follow%20The%20Regularized%20Leader%22%20(,McMahan%20et%20al.%2C%202013.), [4](https://machinelearningmastery.com/gradient-descent-optimization-with-nadam-from-scratch/):\n","\n","**Adagrad:**\n","It adapts the learning rate to the parameters performing small updates for frequently occurring features and large updates for the rarest ones.\n","\n","**Adadelta:**\n","It improves the previous algorithm by introducing a history window which sets a fixed number of past gradients to take in consideration during the training.\n","\n","**RMSprop:**\n","It is very similar to Adadelta. The only difference is in the way they manage the past gradients.\n","\n","**Adam:**\n","It adds to the advantages of Adadelta and RMSprop, the storing of an exponentially decaying average of past gradients similar to momentum.\n","\n","**AdaMax:**\n","is an extension to the Adam version of gradient descent that generalizes the approach to the infinite norm (max) and may result in a more effective optimization on some problems.\n","\n","**Nadam:**\n"," is an extension of the Adam algorithm that incorporates Nesterov momentum and can result in better performance of the optimization algorithm.\n","\n"," **Ftrl:**  is an optimization algorithm developed at Google for click-through rate prediction in the early 2010s. It is most suitable for shallow models with large and sparse feature spaces.\n","\n","\n","In general, each optimizer has its own pros and cons. It is better to start with some simple optimizer algorithm like adam and then move to more complex ones if needed. Adam usually works well in practice. For shallow networks SGD and Nadam may be used and for deep networks Adam or RMSprop may be used. [reference](https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/). Based on the above results, we will use adamax for this project.\n","\n","\n"],"metadata":{"id":"w_H6A5-rzocQ"}},{"cell_type":"markdown","source":["**Tune hyperparameters (training testing and validation)**\n","\n","So far we have ivestigated the effect of batch size and optimizer on the dev set and have decided to stick with Adamax optimizer and batch size of 64. Now for further tuning the hyperparameters, given that our neural architecture is already performing well, we keep it fixed and will try different optimizer hyerparameters on the validation set.\n","\n","Note that we use greed search for serching the hyperparameter space. For each combination, we train a cloned version of model for 30 epochs and record theb est achieved result on validation set and use it for comparison.\n"],"metadata":{"id":"GEHEZUYtBy73"}},{"cell_type":"code","source":["learning_rates=[0.001, 0.0001]\n","beta_1s=[0.9, 0.99]\n","beta_2s=[0.999, 0.9999]\n","epsilons=[1e-6, 1e-7]\n","best_loss_found=10000000\n","best_hyperparameters_found={}\n","EPOCHS=20\n","\n","for learning_rate in learning_rates:\n","  for beta_1 in beta_1s:\n","    for beta_2 in beta_2s:\n","      for epsilon in epsilons:\n","        optimizer=tf.keras.optimizers.Adamax(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, name=\"Adamax\")\n","        cloned_model=copy.deepcopy(model)\n","        cloned_model.compile(optimizer=optimizer, loss='mse')\n","        cloned_model.fit(generatorTrain, epochs=EPOCHS, batch_size=optimal_batch_size, shuffle=False, validation_data=generatorValidation, verbose=0, callbacks=[callback])\n","        loss=cloned_model.evaluate(generatorValidation, verbose=0)\n","\n","        if loss < best_loss_found:\n","          best_loss_found=loss\n","          best_hyperparameters_found['b1']=beta_1\n","          best_hyperparameters_found['b2']=beta_2\n","          best_hyperparameters_found['epsilon']=epsilon\n","          best_hyperparameters_found['learning_rate']=learning_rate\n","        \n","        print(\"Achieved loss of \", loss, \" with hyper parameters: \", learning_rate,\" \", beta_1, \" \", beta_2, \" \", epsilon)\n","\n","    \n","\n","print(\"best ahieved hyeperparameters are : lr= \", best_hyperparameters_found['learning_rate'], \" beta_1= \", best_hyperparameters_found['b1'], ' beta_2= ', best_hyperparameters_found['b2'], \" epsilon= \",best_hyperparameters_found['epsilon'])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1R8JjaGC83N","executionInfo":{"status":"ok","timestamp":1668919126950,"user_tz":360,"elapsed":981842,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"baeda27d-484d-40e2-a5a3-272753f5d6b4"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Achieved loss of  0.002700971672311425  with hyper parameters:  0.001   0.9   0.999   1e-06\n","Achieved loss of  0.001054560299962759  with hyper parameters:  0.001   0.9   0.999   1e-07\n","Achieved loss of  0.0015999102033674717  with hyper parameters:  0.001   0.9   0.9999   1e-06\n","Achieved loss of  0.003022223711013794  with hyper parameters:  0.001   0.9   0.9999   1e-07\n","Achieved loss of  0.0028842175379395485  with hyper parameters:  0.001   0.99   0.999   1e-06\n","Achieved loss of  0.003979772794991732  with hyper parameters:  0.001   0.99   0.999   1e-07\n","Achieved loss of  0.003488096408545971  with hyper parameters:  0.001   0.99   0.9999   1e-06\n","Achieved loss of  0.004031605552881956  with hyper parameters:  0.001   0.99   0.9999   1e-07\n","Achieved loss of  0.0053725712932646275  with hyper parameters:  0.0001   0.9   0.999   1e-06\n","Achieved loss of  0.004588702693581581  with hyper parameters:  0.0001   0.9   0.999   1e-07\n","Achieved loss of  0.004091124515980482  with hyper parameters:  0.0001   0.9   0.9999   1e-06\n","Achieved loss of  0.004660998471081257  with hyper parameters:  0.0001   0.9   0.9999   1e-07\n","Achieved loss of  0.003508275840431452  with hyper parameters:  0.0001   0.99   0.999   1e-06\n","Achieved loss of  0.003904628800228238  with hyper parameters:  0.0001   0.99   0.999   1e-07\n","Achieved loss of  0.003937812056392431  with hyper parameters:  0.0001   0.99   0.9999   1e-06\n","Achieved loss of  0.004201879724860191  with hyper parameters:  0.0001   0.99   0.9999   1e-07\n","best ahieved hyeperparameters are : lr=  0.001  beta_1=  0.9  beta_2=  0.999  epsilon=  1e-07\n"]}]},{"cell_type":"code","source":["learning_rates=[0.001, 0.01]\n","beta_1s=[0.9, 0.99]\n","beta_2s=[0.999, 0.9999]\n","epsilons=[1e-6, 1e-7]\n","best_loss_found=10000000\n","best_hyperparameters_found={}\n","EPOCHS=20\n","\n","for learning_rate in learning_rates:\n","  for beta_1 in beta_1s:\n","    for beta_2 in beta_2s:\n","      for epsilon in epsilons:\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon, name=\"adam\")\n","        cloned_model=copy.deepcopy(model)\n","        cloned_model.compile(optimizer=optimizer, loss='mse')\n","        cloned_model.fit(generatorTrain, epochs=EPOCHS, batch_size=optimal_batch_size, shuffle=False, validation_data=generatorValidation, verbose=1, callbacks=[callback])\n","        loss=cloned_model.evaluate(generatorValidation, verbose=0)\n","\n","        if loss < best_loss_found:\n","          best_loss_found=loss\n","          best_hyperparameters_found['b1']=beta_1\n","          best_hyperparameters_found['b2']=beta_2\n","          best_hyperparameters_found['epsilon']=epsilon\n","          best_hyperparameters_found['learning_rate']=learning_rate\n","        \n","        print(\"Achieved loss of \", loss, \" with hyper parameters: \", learning_rate,\" \", beta_1, \" \", beta_2, \" \", epsilon)\n","\n","    \n","\n","print(\"best ahieved hyeperparameters are : lr= \", best_hyperparameters_found['learning_rate'], \" beta_1= \", best_hyperparameters_found['b1'], ' beta_2= ', best_hyperparameters_found['b2'], \" epsilon= \",best_hyperparameters_found['epsilon'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FBmzNBybTQ-d","executionInfo":{"status":"ok","timestamp":1668920353199,"user_tz":360,"elapsed":1089360,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"e94225a1-c7e8-4e2d-f0d6-e459adaff4af"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0049 - val_loss: 0.0057\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0053 - val_loss: 0.0023\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 9.9592e-04\n","Epoch 4/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0056 - val_loss: 0.0048\n","Epoch 5/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0046 - val_loss: 0.0033\n","Epoch 6/20\n","442/442 [==============================] - 2s 5ms/step - loss: 0.0045 - val_loss: 0.0020\n","Epoch 7/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0047 - val_loss: 1.8528e-04\n","Epoch 8/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0048 - val_loss: 0.0062\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0043 - val_loss: 0.0011\n","Epoch 10/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0047 - val_loss: 0.0119\n","Epoch 11/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0043 - val_loss: 0.0010\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0049 - val_loss: 0.0037\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0043 - val_loss: 0.0011\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0052 - val_loss: 0.0025\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0056 - val_loss: 0.0089\n","Epoch 16/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0050 - val_loss: 0.0034\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 0.0057\n","Epoch 18/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0052 - val_loss: 0.0061\n","Epoch 19/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0049 - val_loss: 7.8779e-04\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0051 - val_loss: 1.8326e-04\n","Achieved loss of  0.0001832552661653608  with hyper parameters:  0.001   0.9   0.999   1e-06\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0046 - val_loss: 0.0074\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0047 - val_loss: 2.6290e-04\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0047 - val_loss: 0.0082\n","Epoch 4/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0046 - val_loss: 0.0055\n","Epoch 5/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0048 - val_loss: 2.6161e-04\n","Epoch 6/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0047 - val_loss: 5.4785e-04\n","Epoch 7/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0044 - val_loss: 1.5199e-04\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 0.0033\n","Epoch 9/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0043 - val_loss: 0.0014\n","Epoch 10/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0056 - val_loss: 0.0059\n","Epoch 11/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0048 - val_loss: 5.0243e-04\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0051 - val_loss: 9.3578e-04\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0046 - val_loss: 0.0029\n","Epoch 14/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0047 - val_loss: 0.0013\n","Epoch 15/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0047 - val_loss: 0.0023\n","Epoch 16/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0049 - val_loss: 9.4401e-04\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0050 - val_loss: 0.0011\n","Epoch 18/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0052 - val_loss: 0.0012\n","Epoch 19/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0044 - val_loss: 0.0021\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0045 - val_loss: 9.8704e-04\n","Achieved loss of  0.0009870417416095734  with hyper parameters:  0.001   0.9   0.999   1e-07\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0050 - val_loss: 1.9766e-04\n","Epoch 2/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0051 - val_loss: 0.0081\n","Epoch 3/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0047 - val_loss: 8.1400e-04\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 0.0080\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0044 - val_loss: 0.0022\n","Epoch 6/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0048 - val_loss: 0.0011\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 6.2851e-04\n","Epoch 8/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0049 - val_loss: 5.7731e-04\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0052 - val_loss: 0.0029\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0055 - val_loss: 0.0028\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0046 - val_loss: 0.0023\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0047 - val_loss: 1.5932e-04\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0050 - val_loss: 0.0016\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0053 - val_loss: 6.2340e-04\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0050 - val_loss: 7.1590e-04\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0046 - val_loss: 0.0030\n","Epoch 17/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0045 - val_loss: 0.0019\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0044 - val_loss: 0.0016\n","Epoch 19/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0045 - val_loss: 1.6157e-04\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0046 - val_loss: 0.0081\n","Achieved loss of  0.008103256113827229  with hyper parameters:  0.001   0.9   0.9999   1e-06\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0051 - val_loss: 1.7561e-04\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0053 - val_loss: 0.0056\n","Epoch 3/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0047 - val_loss: 0.0136\n","Epoch 4/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0048 - val_loss: 0.0041\n","Epoch 5/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0048 - val_loss: 0.0028\n","Epoch 6/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0050 - val_loss: 0.0064\n","Epoch 7/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0046 - val_loss: 0.0018\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0038 - val_loss: 0.0043\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 2.3208e-04\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0049 - val_loss: 0.0098\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0054 - val_loss: 0.0028\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0046 - val_loss: 0.0012\n","Epoch 13/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0042 - val_loss: 5.4273e-04\n","Epoch 14/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0046 - val_loss: 0.0109\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0045 - val_loss: 0.0032\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0047 - val_loss: 0.0043\n","Epoch 17/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0046 - val_loss: 0.0028\n","Epoch 18/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0051 - val_loss: 0.0029\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0049 - val_loss: 0.0010\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0041 - val_loss: 0.0012\n","Achieved loss of  0.0012398437829688191  with hyper parameters:  0.001   0.9   0.9999   1e-07\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0045 - val_loss: 0.0041\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0073 - val_loss: 5.2011e-04\n","Epoch 3/20\n","442/442 [==============================] - 3s 7ms/step - loss: 0.0054 - val_loss: 0.0100\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0073 - val_loss: 2.9213e-04\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0057 - val_loss: 2.5793e-04\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0130 - val_loss: 2.4151e-04\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0365 - val_loss: 0.0702\n","Epoch 8/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0138 - val_loss: 0.0142\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0081 - val_loss: 0.0128\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0062 - val_loss: 0.0077\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0068 - val_loss: 0.0078\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0072 - val_loss: 0.0022\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0083 - val_loss: 0.0097\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0111 - val_loss: 0.0048\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0077 - val_loss: 1.5382e-04\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0063 - val_loss: 2.4126e-04\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0072 - val_loss: 0.0139\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0069 - val_loss: 0.0060\n","Epoch 19/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0056 - val_loss: 0.0012\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0109 - val_loss: 0.0014\n","Achieved loss of  0.0013574127806350589  with hyper parameters:  0.001   0.99   0.999   1e-06\n","Epoch 1/20\n","442/442 [==============================] - 7s 10ms/step - loss: 0.0045 - val_loss: 0.0030\n","Epoch 2/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0058 - val_loss: 0.0044\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0063 - val_loss: 1.2149e-04\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0047 - val_loss: 0.0059\n","Epoch 5/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0056 - val_loss: 6.2692e-05\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0056 - val_loss: 0.0018\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0063 - val_loss: 2.3804e-04\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0052 - val_loss: 0.0015\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0053 - val_loss: 5.2167e-04\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0079 - val_loss: 0.0192\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0067 - val_loss: 0.0028\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0083 - val_loss: 5.3291e-04\n","Epoch 13/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0096 - val_loss: 1.0414e-04\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0089 - val_loss: 4.9488e-04\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0160 - val_loss: 3.8250e-04\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0425 - val_loss: 0.0724\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0100 - val_loss: 0.0112\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0056 - val_loss: 0.0025\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0053 - val_loss: 0.0086\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0083 - val_loss: 0.0062\n","Achieved loss of  0.006187275517731905  with hyper parameters:  0.001   0.99   0.999   1e-07\n","Epoch 1/20\n","442/442 [==============================] - 7s 7ms/step - loss: 0.0047 - val_loss: 7.3466e-04\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 0.0070\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0059 - val_loss: 7.3338e-04\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0097 - val_loss: 6.8354e-04\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0065 - val_loss: 5.0441e-04\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0095 - val_loss: 3.0197e-04\n","Epoch 7/20\n","442/442 [==============================] - 4s 8ms/step - loss: 0.0086 - val_loss: 4.3025e-04\n","Epoch 8/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0147 - val_loss: 4.1528e-04\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0321 - val_loss: 0.0617\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0149 - val_loss: 0.0080\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0084 - val_loss: 0.0058\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0075 - val_loss: 0.0099\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0069 - val_loss: 0.0049\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0059 - val_loss: 0.0023\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0064 - val_loss: 0.0130\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0061 - val_loss: 0.0062\n","Epoch 17/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0047 - val_loss: 0.0044\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0066 - val_loss: 0.0022\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0062 - val_loss: 0.0022\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0061 - val_loss: 0.0071\n","Achieved loss of  0.007136670872569084  with hyper parameters:  0.001   0.99   0.9999   1e-06\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0050 - val_loss: 4.4498e-04\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0100 - val_loss: 2.3603e-04\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0048 - val_loss: 0.0053\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0088 - val_loss: 0.0017\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0085 - val_loss: 3.3580e-04\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0074 - val_loss: 3.0612e-04\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0062 - val_loss: 0.0018\n","Epoch 8/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0068 - val_loss: 0.0041\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0060 - val_loss: 0.0024\n","Epoch 10/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0090 - val_loss: 0.0212\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0069 - val_loss: 0.0039\n","Epoch 12/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0059 - val_loss: 4.3170e-04\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0103 - val_loss: 0.0011\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0172 - val_loss: 3.5479e-04\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0255 - val_loss: 0.0275\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0170 - val_loss: 0.0160\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0089 - val_loss: 0.0080\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0077 - val_loss: 0.0042\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0101 - val_loss: 3.8165e-04\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0071 - val_loss: 0.0078\n","Achieved loss of  0.007763156667351723  with hyper parameters:  0.001   0.99   0.9999   1e-07\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0066 - val_loss: 0.0016\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0123 - val_loss: 0.0012\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0120 - val_loss: 0.0023\n","Epoch 4/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0123 - val_loss: 0.0020\n","Epoch 5/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0113 - val_loss: 3.4843e-04\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0092 - val_loss: 0.0010\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0088 - val_loss: 7.1156e-04\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0070 - val_loss: 0.0039\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0068 - val_loss: 4.5896e-04\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0068 - val_loss: 4.0952e-04\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0072 - val_loss: 9.6431e-04\n","Epoch 12/20\n","442/442 [==============================] - 3s 7ms/step - loss: 0.0067 - val_loss: 0.0026\n","Epoch 13/20\n","442/442 [==============================] - 3s 7ms/step - loss: 0.0068 - val_loss: 0.0093\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0060 - val_loss: 0.0051\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0058 - val_loss: 6.4509e-04\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0083 - val_loss: 0.0037\n","Epoch 17/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0082 - val_loss: 0.0070\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0068 - val_loss: 0.0070\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0064 - val_loss: 6.7614e-04\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0066 - val_loss: 0.0019\n","Achieved loss of  0.0018693332094699144  with hyper parameters:  0.01   0.9   0.999   1e-06\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0047 - val_loss: 3.9591e-04\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0137 - val_loss: 0.0058\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0131 - val_loss: 0.0030\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0195 - val_loss: 5.9242e-04\n","Epoch 5/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0131 - val_loss: 4.3236e-04\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0141 - val_loss: 4.2457e-04\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0095 - val_loss: 0.0024\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0088 - val_loss: 0.0177\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0063 - val_loss: 0.0018\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0072 - val_loss: 0.0019\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0064 - val_loss: 0.0066\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0069 - val_loss: 0.0015\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0072 - val_loss: 0.0107\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0064 - val_loss: 0.0013\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0075 - val_loss: 0.0046\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0065 - val_loss: 0.0074\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0059 - val_loss: 3.6227e-04\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0076 - val_loss: 0.0068\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0067 - val_loss: 0.0016\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0068 - val_loss: 0.0016\n","Achieved loss of  0.001635683816857636  with hyper parameters:  0.01   0.9   0.999   1e-07\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0076 - val_loss: 0.0013\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0173 - val_loss: 7.4188e-04\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0146 - val_loss: 0.0037\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0126 - val_loss: 0.0022\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0107 - val_loss: 0.0037\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0083 - val_loss: 0.0069\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0083 - val_loss: 0.0117\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0072 - val_loss: 6.3166e-04\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0076 - val_loss: 0.0026\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0065 - val_loss: 0.0029\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0061 - val_loss: 0.0038\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0063 - val_loss: 0.0016\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0064 - val_loss: 0.0130\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0052 - val_loss: 7.5126e-04\n","Epoch 15/20\n","442/442 [==============================] - 2s 6ms/step - loss: 0.0063 - val_loss: 0.0041\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0060 - val_loss: 3.6013e-04\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0069 - val_loss: 0.0010\n","Epoch 18/20\n","442/442 [==============================] - 3s 8ms/step - loss: 0.0068 - val_loss: 0.0230\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0050 - val_loss: 0.0060\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0055 - val_loss: 0.0035\n","Achieved loss of  0.003540819277986884  with hyper parameters:  0.01   0.9   0.9999   1e-06\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0124 - val_loss: 0.0091\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0105 - val_loss: 3.3718e-04\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0087 - val_loss: 0.0116\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0071 - val_loss: 7.2532e-04\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0076 - val_loss: 0.0038\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0056 - val_loss: 0.0050\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0068 - val_loss: 0.0076\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0064 - val_loss: 0.0038\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0065 - val_loss: 0.0028\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0057 - val_loss: 0.0011\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0059 - val_loss: 0.0012\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0058 - val_loss: 0.0016\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0063 - val_loss: 0.0076\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0059 - val_loss: 0.0013\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0060 - val_loss: 0.0144\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0055 - val_loss: 0.0012\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0068 - val_loss: 6.8264e-04\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0060 - val_loss: 0.0053\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0058 - val_loss: 0.0026\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0056 - val_loss: 0.0014\n","Achieved loss of  0.0013661319389939308  with hyper parameters:  0.01   0.9   0.9999   1e-07\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0137 - val_loss: 2.5517e-04\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0416 - val_loss: 0.0183\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0692 - val_loss: 0.0077\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1095 - val_loss: 0.0019\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1405 - val_loss: 0.0379\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1439 - val_loss: 0.0036\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0856 - val_loss: 0.0020\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2331 - val_loss: 0.0141\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3014 - val_loss: 0.0053\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3318 - val_loss: 2.1244e-04\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2748 - val_loss: 0.0014\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1602 - val_loss: 0.0092\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0766 - val_loss: 0.0425\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0309 - val_loss: 0.0218\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0201 - val_loss: 0.0015\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0218 - val_loss: 0.0607\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0207 - val_loss: 0.0278\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0220 - val_loss: 0.0015\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0236 - val_loss: 0.0080\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0242 - val_loss: 0.0188\n","Achieved loss of  0.018788250163197517  with hyper parameters:  0.01   0.99   0.999   1e-06\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0137 - val_loss: 0.0081\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0509 - val_loss: 0.0023\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1407 - val_loss: 0.0057\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1286 - val_loss: 0.0087\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1259 - val_loss: 3.4479e-04\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2003 - val_loss: 0.0212\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1164 - val_loss: 0.0065\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2828 - val_loss: 0.0042\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3320 - val_loss: 0.0012\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3405 - val_loss: 0.0111\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3339 - val_loss: 0.0221\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3266 - val_loss: 0.0293\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3220 - val_loss: 0.0333\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3195 - val_loss: 0.0354\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3182 - val_loss: 0.0365\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3175 - val_loss: 0.0371\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3172 - val_loss: 0.0374\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3170 - val_loss: 0.0375\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3169 - val_loss: 0.0376\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3168 - val_loss: 0.0377\n","Achieved loss of  0.03766871243715286  with hyper parameters:  0.01   0.99   0.999   1e-07\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0200 - val_loss: 0.0012\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0648 - val_loss: 3.7072e-04\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1029 - val_loss: 0.0413\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0657 - val_loss: 0.0212\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2096 - val_loss: 0.0237\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2340 - val_loss: 0.0226\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2674 - val_loss: 0.0172\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2930 - val_loss: 0.0101\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3096 - val_loss: 0.0044\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3192 - val_loss: 0.0012\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3241 - val_loss: 3.4386e-04\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3262 - val_loss: 0.0010\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3267 - val_loss: 0.0026\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3264 - val_loss: 0.0046\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3258 - val_loss: 0.0067\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3250 - val_loss: 0.0088\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3242 - val_loss: 0.0109\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3234 - val_loss: 0.0128\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3227 - val_loss: 0.0146\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3220 - val_loss: 0.0163\n","Achieved loss of  0.016261490061879158  with hyper parameters:  0.01   0.99   0.9999   1e-06\n","Epoch 1/20\n","442/442 [==============================] - 6s 7ms/step - loss: 0.0137 - val_loss: 0.0112\n","Epoch 2/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0487 - val_loss: 0.0057\n","Epoch 3/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.0980 - val_loss: 0.0032\n","Epoch 4/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1244 - val_loss: 0.0010\n","Epoch 5/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.1529 - val_loss: 0.0201\n","Epoch 6/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2387 - val_loss: 0.0244\n","Epoch 7/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2653 - val_loss: 0.0184\n","Epoch 8/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.2900 - val_loss: 0.0112\n","Epoch 9/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3074 - val_loss: 0.0052\n","Epoch 10/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3179 - val_loss: 0.0016\n","Epoch 11/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3233 - val_loss: 3.7167e-04\n","Epoch 12/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3258 - val_loss: 7.7275e-04\n","Epoch 13/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3266 - val_loss: 0.0022\n","Epoch 14/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3264 - val_loss: 0.0040\n","Epoch 15/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3259 - val_loss: 0.0061\n","Epoch 16/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3252 - val_loss: 0.0083\n","Epoch 17/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3244 - val_loss: 0.0103\n","Epoch 18/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3236 - val_loss: 0.0123\n","Epoch 19/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3228 - val_loss: 0.0141\n","Epoch 20/20\n","442/442 [==============================] - 3s 6ms/step - loss: 0.3221 - val_loss: 0.0157\n","Achieved loss of  0.015743009746074677  with hyper parameters:  0.01   0.99   0.9999   1e-07\n","best ahieved hyeperparameters are : lr=  0.001  beta_1=  0.9  beta_2=  0.999  epsilon=  1e-06\n"]}]},{"cell_type":"markdown","source":["The above validation loss is quite high. Although we still can extract the best hyperparameters, it is wise to switch to adam and do the hyper parameter tuning again. It can be seen that lower loss values can be obtained with adam."],"metadata":{"id":"oTcNVb52TBim"}},{"cell_type":"markdown","source":["**Training the final model**\n","\n","Now that we have a model and have found its optimal hyper-parameters, it is now time to train the original model for many more epochs and evaluate it on the test data.\n","\n","**Important notice:**\n","\n","The original dataset consists of nearly 1000 counties of the State of California and the whole state of California itself. First of all, we only used the seasonally adjusted version as it tries to remove the effect of season and let us extract more reliable pattern. \n","\n","The second point is that we decided to train a seperate model for each county. That is, we did not consider each area as a feature. There are two justifications for it. First of all, the dataset was incomplete and for a vast majority of counties, we only had unemployment rates from early 90's. Several counties and the whole state had data from early 70's. So, since the data is incomplete it would not make sense to train the model on earlier dates since an accurate extrapolation is impossible. Furthermore, if we only used the latest available date and treated all areas as a feature for now that we have data for all of them, the problem would be that we have discarded a great portion of our data.\n","\n","The third justification is that each county might have its own unemployment pattern, and merging several counties will not make sense since we are trying to learn one function from multiple functions with different behaviors!\n","\n","\n","With above in mind, we decided to train a seperate model for each area name. This notebook deals with the State of California, for which the greatest number of data is possible.\n","\n","A drawback to this approach is that we might be neglecting the effect of nearby counties on the unemployment rate, since people might migrate. But this is not the case. First of all, such a pattern would also be reflected in the total workforce and by considering that as a feature we have addressed the migration issue. And second, as seen from the results, the models are performing quite well on validation set (and at the end of this notebook we will see that it also performs well on test set). So we also have a good model with low test loss so it also verifies our assumptions!\n","\n","In the next notebook, we will train a seperate model for the reamining area names. What follows will be training of the model for the State of California. For other area names, the same model with the same hyper parameters will be used except that we split data only in train and test partitions and we don't have any validation set as we have already chosen our hyper parameters from the model for State of California."],"metadata":{"id":"yiTD9RXqHToI"}},{"cell_type":"code","source":["EPOCHS=300\n","callback= tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", min_delta=0, patience=15, verbose=0, mode=\"auto\", baseline=None, restore_best_weights=True,)\n","optimizer=tf.keras.optimizers.Adam(learning_rate= best_hyperparameters_found['learning_rate'], beta_1= best_hyperparameters_found['b1'], beta_2=best_hyperparameters_found['b2'], epsilon=best_hyperparameters_found['epsilon'], name=\"adam\")\n","cloned_model=copy.deepcopy(model)\n","model.compile(optimizer='adam', loss='mse')\n","history= model.fit(generatorTrain, epochs=EPOCHS, batch_size=16, shuffle=False, validation_data=generatorValidation, verbose=1, callbacks=[callback])\n","loss=cloned_model.evaluate(generatorValidation, verbose=0)\n","\n","\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('MSE loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"qEai_OKYZUyy","executionInfo":{"status":"ok","timestamp":1668921811319,"user_tz":360,"elapsed":62275,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"6a425b12-d963-4d86-98c2-8bad5567b6a9"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/300\n","442/442 [==============================] - 4s 5ms/step - loss: 0.0027 - val_loss: 2.1895e-04\n","Epoch 2/300\n","442/442 [==============================] - 2s 4ms/step - loss: 0.0017 - val_loss: 3.2456e-04\n","Epoch 3/300\n","442/442 [==============================] - 2s 4ms/step - loss: 0.0032 - val_loss: 3.5601e-04\n","Epoch 4/300\n","442/442 [==============================] - 2s 4ms/step - loss: 0.0019 - val_loss: 2.0152e-04\n","Epoch 5/300\n","442/442 [==============================] - 2s 4ms/step - loss: 9.2999e-04 - val_loss: 1.4229e-04\n","Epoch 6/300\n","442/442 [==============================] - 2s 5ms/step - loss: 9.3111e-04 - val_loss: 8.7614e-05\n","Epoch 7/300\n","442/442 [==============================] - 2s 4ms/step - loss: 6.5110e-04 - val_loss: 3.8463e-05\n","Epoch 8/300\n","442/442 [==============================] - 2s 5ms/step - loss: 5.5157e-04 - val_loss: 3.5274e-05\n","Epoch 9/300\n","442/442 [==============================] - 2s 4ms/step - loss: 4.9982e-04 - val_loss: 3.1125e-05\n","Epoch 10/300\n","442/442 [==============================] - 2s 4ms/step - loss: 4.7331e-04 - val_loss: 3.4360e-05\n","Epoch 11/300\n","442/442 [==============================] - 2s 5ms/step - loss: 4.7875e-04 - val_loss: 3.4510e-05\n","Epoch 12/300\n","442/442 [==============================] - 2s 5ms/step - loss: 4.4215e-04 - val_loss: 3.7723e-05\n","Epoch 13/300\n","442/442 [==============================] - 3s 6ms/step - loss: 4.4040e-04 - val_loss: 4.3502e-05\n","Epoch 14/300\n","442/442 [==============================] - 2s 4ms/step - loss: 4.1785e-04 - val_loss: 5.1298e-05\n","Epoch 15/300\n","442/442 [==============================] - 2s 4ms/step - loss: 4.2223e-04 - val_loss: 4.9107e-05\n","Epoch 16/300\n","442/442 [==============================] - 2s 4ms/step - loss: 4.1880e-04 - val_loss: 5.9094e-05\n","Epoch 17/300\n","442/442 [==============================] - 2s 4ms/step - loss: 4.2222e-04 - val_loss: 5.2147e-05\n","Epoch 18/300\n","442/442 [==============================] - 2s 5ms/step - loss: 4.0663e-04 - val_loss: 1.1757e-04\n","Epoch 19/300\n","442/442 [==============================] - 3s 6ms/step - loss: 4.7558e-04 - val_loss: 5.6792e-05\n","Epoch 20/300\n","442/442 [==============================] - 3s 6ms/step - loss: 3.9578e-04 - val_loss: 8.4290e-05\n","Epoch 21/300\n","442/442 [==============================] - 2s 4ms/step - loss: 4.4271e-04 - val_loss: 5.0017e-05\n","Epoch 22/300\n","442/442 [==============================] - 2s 4ms/step - loss: 3.8140e-04 - val_loss: 1.1899e-04\n","Epoch 23/300\n","442/442 [==============================] - 2s 5ms/step - loss: 4.2843e-04 - val_loss: 3.9027e-05\n","Epoch 24/300\n","442/442 [==============================] - 2s 5ms/step - loss: 4.8312e-04 - val_loss: 1.6881e-04\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xc1Znw8d8zM+qS1d1VbNkYm2ZhYQiEHoIhxWRDMWkkYUPYhQ15k91PYHffFN6QJdlN2GQ3PbAhhGBYIMFJKAFDIITqBtgG27KRLLnKalbXlOf941zZY3kkq8xoZOn5fj7zmTvn3nvumbE8z5x6RVUxxhhjRsuX7AIYY4yZGCygGGOMiQsLKMYYY+LCAooxxpi4sIBijDEmLiygGGOMiQsLKMaMMREpFxEVkcAQjv20iLw42nyMGQsWUIwZhIjUiEiviBT1S1/vfZmXJ6dkxow/FlCMObZ3gWv7XojIKUBm8opjzPhkAcWYY7sP+FTU6+uAX0UfICK5IvIrEWkQkVoR+VcR8Xn7/CLyHyJyQER2AB+Ice7dIrJHRHaJyDdFxD/cQorITBFZJSJNIlItIp+L2rdURNaIyEER2Sci3/PS00Xk1yLSKCItIvK6iEwb7rWNAQsoxgzFK8AUEVnofdGvAH7d75j/AnKBucD5uAD0GW/f54APApVAFXBlv3N/CYSAed4x7wf+dgTlXAnUAzO9a3xLRC7y9n0f+L6qTgEqgIe89Ou8cpcAhcCNQNcIrm2MBRRjhqivlnIJ8Dawq29HVJC5TVXbVLUG+C7wSe+Qq4H/VNU6VW0C/i3q3GnA5cAXVbVDVfcDd3n5DZmIlADnAF9R1W5V3QD8gsM1qyAwT0SKVLVdVV+JSi8E5qlqWFXXqurB4VzbmD4WUIwZmvuAjwGfpl9zF1AEpAC1UWm1wCxveyZQ129fnzLv3D1ek1ML8FNg6jDLNxNoUtW2AcpwPXAC8I7XrPXBqPf1FLBSRHaLyHdEJGWY1zYGsIBizJCoai2uc/5y4NF+uw/gfumXRaWVcrgWswfXpBS9r08d0AMUqWqe95iiqicNs4i7gQIRyYlVBlXdpqrX4gLVt4GHRSRLVYOq+g1VXQScjWua+xTGjIAFFGOG7nrgIlXtiE5U1TCuT+IOEckRkTLgSxzuZ3kI+IKIzBaRfODWqHP3AH8CvisiU0TEJyIVInL+cAqmqnXAS8C/eR3tp3rl/TWAiHxCRIpVNQK0eKdFRORCETnFa7Y7iAuMkeFc25g+FlCMGSJV3a6qawbY/Q9AB7ADeBH4DXCPt+/nuGalN4B1HF3D+RSQCmwGmoGHgRkjKOK1QDmutvJb4Guq+oy3bxmwSUTacR30K1S1C5juXe8grm/oeVwzmDHDJnaDLWOMMfFgNRRjjDFxYQHFGGNMXFhAMcYYExcWUIwxxsTFpF72uqioSMvLy5NdDGOMOa6sXbv2gKoW90+f1AGlvLycNWsGGgVqjDEmFhGpjZVuTV7GGGPiwgKKMcaYuLCAYowxJi4mdR9KLMFgkPr6erq7u5NdlIRKT09n9uzZpKTYwrLGmPiwgNJPfX09OTk5lJeXIyLJLk5CqCqNjY3U19czZ86cZBfHGDNBWJNXP93d3RQWFk7YYAIgIhQWFk74WpgxZmxZQIlhIgeTPpPhPRpjxpYFlHFMVWnu6CUcsdtTGGPGPwso40xLSws/+tGPAOgJRahr7qS5M3jM8y6//HJaWlqOeZwxxiSKBZRxJjqg9IZczaQnGCYUCg163uOPP05eXl7Cy2eMMQOxUV7jzK233sr27dtZvHgx4gsggRQKCvLZuaOarVu3csUVV1BXV0d3dze33HILN9xwA3B4GZn29nYuu+wy3vve9/LSSy8xa9YsHnvsMTIyMpL8zowxE50FlEF84/eb2Lz7YFzzXDRzCl/70EkD7r/zzjvZuHEjGzZs4H9//xSfuuZvWPXcy1xy5qkA3HPPPRQUFNDV1cUZZ5zBRz/6UQoLC4/IY9u2bTzwwAP8/Oc/5+qrr+aRRx7hE5/4RFzfhzHG9GcBZRwLRSKcvPh0ps0qJRyJ4Pf5+MEPfsBvf/tbAOrq6ti2bdtRAWXOnDksXrwYgCVLllBTUzPWRTfGTEIWUAYxWE1iLATDETIzswDXQf/aSy/wzDPP8PLLL5OZmckFF1wQcy5JWlraoW2/309XV9eYldkYM3lZp/w4k5OTQ1tbGxFVQmEl4HPzRXqCEVpbW8nPzyczM5N33nmHV155JcmlNcaYw6yGMs4UFhZyzjnncMoppyD+VGbOmI4g9ITCLFu2jJ/85CcsXLiQBQsWcNZZZyW7uMYYc4ioarLLkDRVVVXa/wZbb7/9NgsXLkxSiQ5r6w7y7oEOKoqzqW/uIj3FR1lhVlyvMV7eqzHm+CIia1W1qn+6NXmNUz3eHJTUgI+0gI/uoM2WN8aMbwkNKCKyTES2iEi1iNwaY3+aiDzo7X9VRMqj9t3mpW8RkUu9tHQReU1E3hCRTSLyjajj53h5VHt5pibyvSVabyiCT4SAT0hL8dEbjhCZxLVJY8z4l7CAIiJ+4IfAZcAi4FoRWdTvsOuBZlWdB9wFfNs7dxGwAjgJWAb8yMuvB7hIVU8DFgPLRKSvI+HbwF1eXs1e3set3lCE1IAPESEt4EdVCYaslmKMGb8SWUNZClSr6g5V7QVWAsv7HbMcuNfbfhi4WNwyuMuBlarao6rvAtXAUnXaveNTvId651zk5YGX5xWJemNjoScUIS3g/nnSveceCyjGmHEskQFlFlAX9breS4t5jKqGgFagcLBzRcQvIhuA/cDTqvqqd06Ll8dA1zpuqCq9YVdDAUhNcc/doXAyi2WMMYM67jrlVTWsqouB2cBSETl5OOeLyA0iskZE1jQ0NCSmkKMUDCuqSqrf/fMEfD4Cfh891jFvjBnHEhlQdgElUa9ne2kxjxGRAJALNA7lXFVtAZ7D9bE0AnleHgNdq++8n6lqlapWFRcXj+BtJV6vVxPpa/Lq247V5JWdnT1m5TLGmMEkMqC8Dsz3Rl+l4jrZV/U7ZhVwnbd9JfCsuokxq4AV3iiwOcB84DURKRaRPAARyQAuAd7xznnOywMvz8cS+N4Sqjd8eMhwn/SAj55QmMk8b8gYM74lbKa8qoZE5GbgKcAP3KOqm0TkdmCNqq4C7gbuE5FqoAkXdPCOewjYDISAm1Q1LCIzgHu9EV8+4CFV/YN3ya8AK0Xkm8B6L+/jzq233sqUoml86NrPkuL38fWvf51AIMCfnllNQ2MTfiLc8c1vsnx5//ENxhiTXDZTfrCZ8k/cCnvfiu9Fp58Cl9054O7169dz403/wK8efYIF03NYtGgRTz31FL60TBp7/UyRbt53/nvZtm0bIkJ2djbt7e0D5jcYmylvjBmJgWbK21pe40xlZSUHDjTQfGAfb+zbQX5+PtOnT+cLt3yR1c/9mbSUALt27WLfvn1Mnz492cU1xphDLKAMZpCaRKKoKpdcvpyn//gYXa2NXHPNNdx///00NR7gwSeeZ1peFmcvXhhz2XpjjEmm427Y8EQXiijv/9BH+P2jD/Pwww9z1VVX0draytSpU8nOSOPPf36O2traZBfTGGOOYjWUcaY3FGHegoV0dLQxa9YsZsyYwcc//nE+9KEP8aELz2LRqZWceOKJyS6mMcYcxQLKONPrzTVZt/4N0lL8ABQVFfHyyy+z72A3+w52c/LMXHzejbdG2iFvjDHxZk1e40xPKIIAKYGj/2nSbE0vY8w4ZgFlnOkNRUjx+/CJHLUvLeBqLD22ppcxZhyygBJDMufmRC8K2V88ayiTef6RMSYxLKD0k56eTmNjY9K+cHtC4SPW8Irm8wmpcVgkUlVpbGwkPT19VPkYY0w065TvZ/bs2dTX15OMlYgjEWV3azcdGQEO7k2JecyB9h4iEaVj/+iCQXp6OrNnzx5VHsYYE80CSj8pKSnMmTMnKdd+s76Fz933V376ySUsXRh7Fvztv9/MA6/tZNM3Lj000ssYY8YDa/IaR2oaOwEoL8wa8JiKqVl0BcPsOWgz5Y0x44sFlHGk9kAHAKUFmQMeU1Hs7n+yfb/NPzHGjC8WUMaR2qZOpk1JIyPVP+AxhwJKgwUUY8z4YgFlHKlt7KBskOYugKLsVKakByygGGPGHQsoI/CD1du46icvxT3fmsZOygZp7gIQESqmZrN9f0fcr2+MMaNhAWUE/D7h9ZpmWjuDccuzszdEQ1sP5UWD11DANXtZDcUYM95YQBmBypI8ANbXNcctz1pvhFdZ4eA1FHABZX9bDwe74xfQjDFmtCygjMCpJXn4BNbvbIlbnrWNrglrsCHDfSqK3TE7GqzZyxgzflhAGYHstAAnTMthfV08A4qroZQOpYYy1YYOG2PGHwsoI1RZms+Gnc1EIvFZ86umsZOCrFSmpMdeciVaaUEmAZ9YP4oxZlyxgDJClaV5HOwOseNAfJqd3JDhY9dOAFL8PsoKMy2gGGPGFQsoI3R6qeuYX7czPh3ztUMYMhytojjb+lCMMeNKQgOKiCwTkS0iUi0it8bYnyYiD3r7XxWR8qh9t3npW0TkUi+tRESeE5HNIrJJRG6JOv7rIrJLRDZ4j8sT+d7mFmUzJT0Ql475nlCY3a1dx5zUGK1iajY1jR2Ewnb3RmPM+JCwgCIifuCHwGXAIuBaEVnU77DrgWZVnQfcBXzbO3cRsAI4CVgG/MjLLwR8WVUXAWcBN/XL8y5VXew9Hk/UewN3b5LFpfmsj0MNpa6pC1UoLxpeDSUYVuqau0Z9fWOMiYdE1lCWAtWqukNVe4GVwPJ+xywH7vW2HwYuFhHx0leqao+qvgtUA0tVdY+qrgNQ1TbgbWBWAt/DoCpL8ti6r432ntCo8ukbMlxaMIwaijd02EZ6GWPGi0QGlFlAXdTreo7+8j90jKqGgFagcCjnes1jlcCrUck3i8ibInKPiOTHKpSI3CAia0RkzWhvolVZmkdE3X1MRqP20LL1Q6+hzLVFIo0x48xx2SkvItnAI8AXVfWgl/xjoAJYDOwBvhvrXFX9mapWqWpVcXHxqMpRWeJi1mj7UWobO8hJC1CQlTrkc3IzUijOSbOAYowZNxIZUHYBJVGvZ3tpMY8RkQCQCzQOdq6IpOCCyf2q+mjfAaq6T1XDqhoBfo5rckuo3MwUKoqzRt2PUtPYSWlhJq61b+gqirPYbiO9jDHjRCIDyuvAfBGZIyKpuE72Vf2OWQVc521fCTyrquqlr/BGgc0B5gOvef0rdwNvq+r3ojMSkRlRLz8CbIz7O4qhsjSf9TtbcMUemZ1NnUNacqW/iuJsqve3j+raxhgTLwkLKF6fyM3AU7jO84dUdZOI3C4iH/YOuxsoFJFq4EvArd65m4CHgM3Ak8BNqhoGzgE+CVwUY3jwd0TkLRF5E7gQ+D+Jem/RKkvzaOzopa5pZKOtQuEIdU2dQ57UGK2iOJvWriBNHb0jurYxxsRTIJGZe0N3H++X9tWo7W7gqgHOvQO4o1/ai0DMdiFV/eRoyzsSff0o63Y2D2kdrv52t3QTiujIAkrfml4NHRRmpw37fGOMiafjslN+PFkwPYfMVP+I+1Fqm1wfyHAmNfY5NHTYOuaNMeOABZRR8vuE02bnjXjl4ZpDQ4aHH1Bm5maQnuKzuSjGmHHBAkocVJbmsXn3QbqD4WGfW3ugg7SAj6k5w2+y8vmEuUV290ZjzPhgASUOKkvzCUWUjbtah31urdch7/MNb8hwn4qp2TZ02BgzLlhAiYPKUaw87JatH35zV5+K4izqmjtHVDsyxph4soASB0XZaZQWZA57xnwkosNetr6/iuJsVKGm0WopxpjksoASJ5WlecMOKPvbeugJRSgrGk0Npe92wBZQjDHJZQElTipL8th7sJs9rUOf4NhXqxjOopD9zSnKQsSGDhtjks8CSpxUlg5/oci+ZevLhrFsfX8ZqX5m5WVYQDHGJJ0FlDhZOGMKaQEf62qH3jFf09hJwCfMzEsf1bUrim3osDEm+SygxElqwMcps3KHNcFxZ2MnJQWZBPyj+2eoKM5m+/4OIhFbJNIYkzwWUOKosjSPt3a10hsa2n3eaxo7KB3FCK8+FVOz6AqG2Xuwe9R5GWPMSFlAiaPK0nx6QxHe3nPwmMequiHDo+mQ71Nhd280xowDFlDiaDgTHJs6emnvCY1qUmOfw0OHLaAYY5LHAkoczcjNYEZu+pBGeh1aFLJo9DWUouxUpqQHbAkWY0xSWUCJs8rSPNbXHbuG0jdkuHQUQ4b7iAhzbaSXMSbJLKDEWWVJPnVNXTS09Qx6XG1jJyJQUpARl+va0GFjTLJZQImzvn6UDccYPlzb2MHM3AzSAv64XLdiahb7DvbQ1h2MS37GGDNcFlDi7ORZuQR8csyO+ZrGkd1HfiB9HfM7rB/FGJMkFlDiLD3Fz0kzpxzzlsA7mzrjMsKrjw0dNsYkmwWUBKgszefN+lZC4dgTHFu7gjR19MZlDkqfssJMAj6xgGKMSRoLKAlQWZpHZ2+Yrftif7nv9IYMx7PJK8Xvo7Qw05axN8YkTUIDiogsE5EtIlItIrfG2J8mIg96+18VkfKofbd56VtE5FIvrUREnhORzSKySURuiTq+QESeFpFt3nN+It/bYCpLvJWHBxg+XNvkrTIcxyYvsJFexpjkSlhAERE/8EPgMmARcK2ILOp32PVAs6rOA+4Cvu2duwhYAZwELAN+5OUXAr6sqouAs4CbovK8FVitqvOB1d7rpCgpyKAwK5V1tbFHetUmoIYCLqDUNHYM2NRmjDGJlMgaylKgWlV3qGovsBJY3u+Y5cC93vbDwMUiIl76SlXtUdV3gWpgqaruUdV1AKraBrwNzIqR173AFQl6X8ckIlSW5g9YQ6k50EFxThqZqYG4XreiOItgWKlrHvpNvowxJl4SGVBmAXVRr+s5/OV/1DGqGgJagcKhnOs1j1UCr3pJ01R1j7e9F5gWq1AicoOIrBGRNQ0NDcN7R8NQWZrHjoYOWjp7j9pX2xSfRSH7q5hqa3oZY5LnuOyUF5Fs4BHgi6p61NK+qqpAzJuDqOrPVLVKVauKi4sTVsbBJjjWNnbEvf8EoKLIhg4bY5InkQFlF1AS9Xq2lxbzGBEJALlA42DnikgKLpjcr6qPRh2zT0RmeMfMAPbH7Z2MwGmz8/DJ0bcE7uwNse9gD2VxuA9Kf7mZKRRlp1lAMcYkxbACioj4RGTKEA9/HZgvInNEJBXXyb6q3zGrgOu87SuBZ73axSpghTcKbA4wH3jN61+5G3hbVb83SF7XAY8N573FW1ZagAXTpxw1Y35nk9chXxT/Ggq4fhRbddgYkwzHDCgi8hsRmSIiWcBGYLOI/NOxzvP6RG4GnsJ1nj+kqptE5HYR+bB32N1AoYhUA1/CG5mlqpuAh4DNwJPATaoaBs4BPglcJCIbvMflXl53ApeIyDbgfd7rpKoszWNDXcsRt+btG+GViD4UcP0o1fvbcXHZGGPGzlCGGS1S1YMi8nHgCdyX/lrg3491oqo+DjzeL+2rUdvdwFUDnHsHcEe/tBcBGeD4RuDiY5VpLFWW5PGbV3ey40A786bmAIeXrS+Lw7L1sVQUZx+aiV+YnZaQaxhjTCxDafJK8fotrgBWqWqQATq8zZEqS90Ex3VR/Sg1jZ3kZaaQm5mSkGtWFLtAZc1expixNpSA8lOgBsgCXhCRMuDYN003zC3KYkp64IiFInc2xndRyP5skUhjTLIcM6Co6g9UdZaqXq5OLXDhGJTtuOfzeRMcj6ihdCRkhFefWXkZpAV8VNtcFGPMGBtKp/wtXqe8iMjdIrIOuGgMyjYhVJbmsWVfG+09IXpDEXa3dCWsQx5cEDtxxhTeqm9N2DWMMSaWoTR5fdabPPh+IB83yirpI6iOF5Wl+ajCm3Ut1Dd3EtH4LwrZX1VZPm/Ut9AbsjW9jDFjZygBpW9U1eXAfd6Q3pgjrczRFs92M+bX17UkbFHI/qrK8ukJRdi422opxpixM5SAslZE/oQLKE+JSA5gP32HKDczhYriLNbVNlPTmJhl6/tbUu5Gl62tGfyukcYYE09DCSjX4+aenKGqnUAq8JmElmqCOb00/1ANJSvVT1F2akKvNzUnnbLCTF6vaUrodYwxJtpQRnlFcGtp/auI/Adwtqq+mfCSTSCVpfk0dfTywrYGSguzcCvIJNaSsnzW1jbbjHljzJgZyiivO4FbcMugbAa+ICLfSnTBJpK+lYd3NHQkdIRXtDPKC2js6KXG67cxxphEG8rSK5cDi72aCiJyL7Ae+OdEFmwiOWFaDlmpfjp6wwnvP+lTVeb6UV6vaWJOghaiNMaYaENdbTgvajs3EQWZyPw+4bQS9xGOVQ2lojib3IwU65g3xoyZodRQ/g1YLyLP4YYLn0cS79d+vKoszeOl7Y2UjlFA8fmEqrJ81tRax7wxZmwMpVP+AeAs4FHcja3eo6oPJrpgE82yk2awcMYUTpo5dhW8JeX5bG/ooKnj6NsQG2NMvA1YQxGR0/sl1XvPM0VkpqquS1yxJp5TZufyxC3njuk1q8oKAFhb28wli6aN6bWNMZPPYE1e3x1kn2LreY17p87OJcUvrKltsoBijEm4AQOKqtqKwse59BQ/p8zKtY55Y8yYGNY95c3xp6q8gDfrW+kOhpNdFGPMBGcBZYJbUpZPbzjCxl22UKQxJrEsoExwfRMc19Ras5cxJrEGDCgi8omo7XP67bs5kYUy8VOYncbcoizWWD+KMSbBBquhfClq+7/67ftsAspiEsQtFNlkC0UaYxJqsIAiA2zHeh07A5FlIrJFRKpF5KjZ9SKSJiIPevtfFZHyqH23eelbROTSqPR7RGS/iGzsl9fXRWSXiGzwHpcPpYyTQVV5Ps2dQbY3dCS7KMaYCWywgKIDbMd6fRQR8QM/BC4DFgHXisiifoddDzSr6jzgLuDb3rmLgBXAScAy4EdefgC/9NJiuUtVF3uPx49VxsmiqrxvgqMtw2KMSZzBAsqJIvKmiLwVtd33esEQ8l4KVKvqDlXtBVYCy/sdsxy419t+GLhY3M1ClgMrVbVHVd8Fqr38UNUXAPtmHIa5RVkUZKXyuvWjGGMSaLCZ8gtHmfcsoC7qdT1w5kDHqGpIRFqBQi/9lX7nzhrCNW8WkU8Ba4Avq+pR36AicgNwA0BpaenQ3slxTkQ4vdTdcMsYYxJlwBqKqtZGP4B24HSgyHs93vwYqAAWA3sYYOkYVf2ZqlapalVxcfFYli+pzijP590DHRxo70l2UYwxE9Rgw4b/ICIne9szgI240V33icgXh5D3LqAk6vVsLy3mMSISwN1rpXGI5x5BVfepati7EdjP8ZrIjFNV7uajWC3FGJMog/WhzFHVvpFUnwGeVtUP4ZqthjJs+HVgvojMEZFUXCf7qn7HrAKu87avBJ5VN7Z1FbDCGwU2B5gPvDbYxbyg1+cjuABoPCfPyiU14GNNjXU/GWMSY7A+lGDU9sW4X/2oapuIRI6VsdcncjPwFOAH7lHVTSJyO7BGVVcBd+NqPNW4jvYV3rmbROQh3D3sQ8BNqhoGEJEHgAuAIhGpB76mqncD3xGRxbgRaDXA54f4GUwKaQE/p87KtRnzxpiEGSyg1InIP+A6xE8HngQQkQwgZSiZe0N3H++X9tWo7W7gqgHOvQO4I0b6tQMc/8mhlGkyqyov4O4Xd9AdDJOe4j/2CcYYMwyDNXldj5sH8mngGlVt8dLPAv4nweUyCVBVlk8wrLxR13Lsg40xZpgGux/KfuDGGOnPAc8lslAmMZZELRR55tzCJJfGGDPRDHYL4P4d6EdQ1Q/HvzgmkfKzUpk3NdtGehljEmKwPpT34CYdPgC8yhDX7zLjW1VZPk9s3Eskovh89k9qjImfwfpQpgP/DJwMfB+4BDigqs+r6vNjUTgTf0vK8mntClLd0J7sohhjJpjBZsqHVfVJVb0O1xFfDfzZ7oVyfOtbKNLuj2KMibdB79joTSz8G+DXwE3AD4DfjkXBTGKUF2ZSlJ3KGlt52BgTZ4N1yv8K19z1OPCNqFnz5jgmIiwpy7caijEm7garoXwCt+TJLcBLInLQe7SJyMGxKZ5JhKqyAnY2dbK/rTvZRTHGTCCD9aH4VDXHe0yJeuSo6pSxLKSJr0MLRVotxRgTR4P2oZiJ6aSZuaQFfLaulzEmriygTEKpAR+nleTZysPGmLiygDJJVZXls2n3Qbp6w8kuijFmgrCAMkmdUV5AKKJssIUijTFxYgFlkjq91Fso0pq9jDFxYgFlksrNTOGEadnWMW+MiRsLKJNYVXkB63Y2E4losotijJkALKBMYlVl+bR1h9i6vy3ZRTHGTAAWUCaxqjK3UOTrNsHRGBMHFlAmsZKCDKbmpLHWOuaNMXFgAWUSExGqyvOtY94YExcWUCa5JWUF1Dd3sbfVFoo0xoxOQgOKiCwTkS0iUi0it8bYnyYiD3r7XxWR8qh9t3npW0Tk0qj0e0Rkv4hs7JdXgYg8LSLbvOf8RL63iaKqzJuPYvdHMcaMUsICioj4gR8ClwGLgGtFZFG/w64HmlV1HnAX8G3v3EXACuAkYBnwIy8/gF96af3dCqxW1fnAau+1OYZFM6eQkeK3+6MYY0YtkTWUpUC1qu5Q1V5gJbC83zHLgXu97YeBi0VEvPSVqtqjqu/ibj+8FEBVXwBi/ZyOzute4Ip4vpmJKsXvY3FJHmutH8UYM0qJDCizgLqo1/VeWsxjVDUEtAKFQzy3v2mqusfb3gtMi3WQiNwgImtEZE1DQ8NQ3seEV1Wez+Y9B+noCSW7KMaY49iE7JRXVQViTv9W1Z+papWqVhUXF49xycanqvICwrZQpDFmlBIZUHYBJVGvZ3tpMY8RkQCQCzQO8dz+9onIDC+vGcD+EZd8kqkszUME60cxxoxKIIF5vw7MF5E5uGCwAvhYv2NWAdcBLwNXAs+qqorIKuA3IvI9YCbu3vavHeN6fXnd6T0/Fq83MtFNSU9hwbQc/ndtHTWNHagqEXVVPFVFFRT3HDn02u2bnpvOv1y+iIxU/7EuY4yZ4BIWUFQ1JCI3A08BfuAeVYzeeEEAABkNSURBVN0kIrcDa1R1FXA3cJ+IVOM62ld4524SkYeAzUAIuElVwwAi8gBwAVAkIvXA11T1blwgeUhErgdqgasT9d4mohVnlHD3X99lbW0zIuATQQCitvvSwU2KFGD1O/vpCUb496tOS2LpjTHjgbjuhsmpqqpK16xZk+xiHNe+96ct/ODZar5z5alcXVVy7BOMMcc9EVmrqlX90ydkp7wZO7e87wTOrijk//5uI2/vOZjs4hhjksgCihkVv0/4/opKcjNS+Pv719HWHUx2kYwxSWIBxYxacU4a/3VtJTubOrn1kbeYzM2oxkxmFlBMXJw5t5B/fP8C/vjWHn71cm2yi2OMSQILKCZuPn/eXC4+cSrf/ONmmyRpzCRkAcXEjc8nfPfq05iak85N96+jpbM32UUyxowhCygmrvIyU/nRx0+noa2HLz30BpGI9acYM1lYQDFxd1pJHv/6wYU8+85+fvLC9mQXxxgzRiygmIT45FllfPDUGfzHU1t4ZUdjsotjjBkDFlBMQogId370VMoLs/iHB9azv81uMWzMRGcBxSRMdlqAH33idNq6g9zywAbC1p9izIRmAcUk1InTp/D/lp/MyzsauevprckujjEmgSygmIS7qqqEq6tm89/PVfPcFrtNjTETlQUUMyZuX34yJ07P4UsPbmB3S1eyi2OMSQALKGZMpKf4+fEnlhAMKzf9Zh2dvXb/emMmGgsoZszMKcriO1eeyvqdLZxz57N8/5ltNHfYbHpjJgoLKGZMXX7KDB75u/ewpCyfu57Zytl3Psvtv99szWDGTAB2x0a7Y2PSbNnbxk9f2M6qDbsBWL54FjeeP5f503KSXDJjzGAGumOjBRQLKElX39zJL/7yLg++XkdXMMz7Fk7j7y6Yy5KygmQXzRgTgwWUGCygjC9NHb3c+1IN975cQ0tnkKXlBdx4wVwuXDAVEUl28YwxHgsoMVhAGZ86e0OsfK2OX/xlB7tbuzlxeg6fP38ul58yg7SAP9nFM2bSs4ASgwWU8S0YjrBqw25++sJ2tu5rJy3go7I0j6VzCjlzTgGVpXlkpgaSXUxjJp2kBBQRWQZ8H/ADv1DVO/vtTwN+BSwBGoFrVLXG23cbcD0QBr6gqk8NlqeI/BI4H2j1sv+0qm4YrHwWUI4PkYjyl+oD/GVrA6/VNLFxVysRhYBPOGV2LkvnFHDmnAKqyguYkp6S7OIaM+GNeUARET+wFbgEqAdeB65V1c1Rx/w9cKqq3igiK4CPqOo1IrIIeABYCswEngFO8E6LmacXUP6gqg8PtYwWUI5Pbd1B1tY289q7Tbz2bhNv1LcQDCsisGjGlEMB5ozyAgqz05JdXGMmnIECSiLbC5YC1aq6wyvASmA5sDnqmOXA173th4H/Ftf7uhxYqao9wLsiUu3lxxDyNBNcTnoKFyyYygULpgLQHQyzbufhAPPAazv5n7/WADArL4PZ+RnMzs9kVn7fdgYl+ZlMz00nxW9Tsczk01eRiPdgl0QGlFlAXdTreuDMgY5R1ZCItAKFXvor/c6d5W0PlucdIvJVYDVwqxeQjiAiNwA3AJSWlg7zLZnxKD3Fz9kVRZxdUQRAbyjCW7taefXdRqr3tVPf3MXL2w+w52A30RVyn8CM3IyooOMCT2F2KsGwEgxH6A1F3LO33RuOEAwpveEwwbAeSlOF8sJMTpiewwnTcpiZm24j08YRVeXtPW1s29/GufOLKchKTXaRxpSqsrOpk5e2N/LS9kZe3t7IL66rYnFJXlyvM5F6NG8D9gKpwM+ArwC39z9IVX/m7aeqqmryjkiYwFIDPpaU5bOkLP+I9N5QhL2t3dQ3d1Lf3HX4uaWLV99t4ncbuhjqLVv8PiHV7yM14CPF70NVaYxaRiYnLcD8adks8ALMgmk5nDA9hyJrghszqsqGuhae3LiXJzftpbaxE4AUv3DxidO4cslszl9QPGFrqbtbunj5UAA5wO5Wd5O7qTlpnDOvkNQEvO9EBpRdQEnU69leWqxj6kUkAOTiOucHOzdmuqru8dJ6ROR/gH+Mw3swE0hqwEdpYSalhZkx9wfDLuA0dvSS4hfSvGCRGvWc6nfbft/RtY/WziBb97exZW8bW/e55yc27uWB1w5XqguzUjlhWg4nTMtm/rQcpmSk4BPwieAT1wThF8Hnc9t96T4RxHvOTgswPTedgsxUfDHKMZmFI8qamiae2LiXpzbtZU9rNwGfcPa8Im48v4ITpuXwxFt7+N2GXTy5aS9F2aksXzyLK5fMZuGMKXEpw57WLt6oa6U4J43FJXkx/1YSoaGth1d2HA4gNV4Azc9M4T0Vhfzd3ELeU1FERXFWwmrPieyUD+A60C/Gfem/DnxMVTdFHXMTcEpUp/zfqOrVInIS8BsOd8qvBuYDMlCeIjJDVfd4fTB3Ad2qeutgZbROeZNoqkpDew9b97azZV8bW/e2ued9bXT2hkeVd6rfx9QpaUyfks603HSmT0k/anvqlDTSU449dycccU18wXCEUFgJRiJEIuDzQcDnAmjAJ0c8D/dLKRJx+YYjSiiihMPec0RJC/jIzUgZUYAMhiO8vL2RJzft5U+b9nKgvZe0gI/zTijmspOnc/GJ08jNTDnqnOe3NPDw2npWv7OPYFg5aeYUrlwym+WLZw25Saw7GGbT7lbW1bawvq6Z9Ttb2NN6+HbX+ZkpnHdCMRcsKOa8+cVxGySiquxu7Wb9zmbW1DTz0vYDbN3XDrja8ZlzC3hPRRFnVxSyYFpO3H94JGvY8OXAf+KG+N6jqneIyO3AGlVdJSLpwH1AJdAErIjqcP8X4LNACPiiqj4xUJ5e+rNAMS7obABuVNX2wcpnAcUkSySi7DnYTVdvGFUlou5LPaKKKkRUvQdH7T/YFWTvwW72HuxmX6v3fLCHva3ddAWPDlL5mSnkZaYeChahSIRgWAmF3XMwEmEkXwN+L7Ck9AUavw+fCKqHA0UoKoAc6xoikJuRQn5mKnmZKeQd2k517yErNSothb2t3TyxcS/PvL2P1q4gmal+LjxxKpedPJ0LF0wlK21oDTBNHb2s2rCLR9bt4q1draT4hQsXTOXKJbO58MSph5rEVJX65i7W7XSBY/3OZjbvOUgw7N5YSUEGlSX5nF6ax6kleexq7uLPWxp4fut+DrT3IgKnzc7jggXFXLhgKqfMyh3yF313MMxbu1pZV+uuvW5nM/vbXBdxRoqfM+YUcHZFIe+ZW8hJM6cQSHAznk1sjMECiplIVJWD3SH2Hexmb+uRAedgd4gUnxDwuy/+FJ+Q4ve5bb8Q8PlICQgpPt+hYwI+IXwoMCjhSOSomkUwEjnidSii+Aes1Xh5R6f5ffhF6A6GaenspbkzSHNnL61d7rm5I0hrV5D2ntj3z5mSHuB9i6Zx2ckzOHd+0ZBqY4N5Z+9BHllbz2/X7+JAey+FWam8/6TpHGjvYf3OFg60H/4SP3V2LqeX5VNZksfi0jym5qTHzDMSUTbubuXPWxp4bst+NtS1oOqaP88/oZgLTpzKefOLyMtMPfTvWNfUxfq6ZhdA6lrYvPsgIa+Dr6wwk8qSPO/a+Zw4I2fM+4EsoMRgAcWY40NvKEJLVy8tnUFavKCTlRpg6ZwCUgPx/zINhiO8sLWvSWw/s/IyqCzJo9ILICdOzxlxLaCpo5e/bGvguXf28/zWBpo7g/gEKkvzyc9MZUNdMwfa3QCPzFQ/p83O4/SyPCpL8qkszRsXc6ssoMRgAcUYcyyqmrBO7HBEebO+hee2NPD8lv2094RYXJJ/KIAsmJ4zZp36w5GMiY1mqFShbQ80VnuP7d6jGtr2wpLr4OKvQiD5v0yMmWwSOZ/I7xMqS/OpLM3nS5eccOwTxjkLKGOps+lwoOh7NG2Hxh0Q7Dh8XCAdCipg6kKYdhK8/N9Q8xf46D1QNC955TfGmEFYQBkLu9fD89+BLY8fThM/5JdDYQWUn+ueC+e5R85MN16zzzt/hMdugp+eB5f/Oyz+mBsSY4wx44gFlETatdYFkq1PQnoenPuPMPsMFzTyy8A/xJVxT/wAzKyER2+Ax/4etq+GD94F6bmJLb8xxgyDBZREqF8Df74Tqp+GjHy46F9h6echfRQzcafMhE89Bi/eBc99C+pfd01gJWfEr9zGGDMKFlDiqe41F0i2r4aMArj4a7D0c5CWE5/8fX447x9hznnwyPVwz6Vw4T/De/+P22eMMUlkASUeal+G5++EHX+GzCJ43zfgjL+FtOzEXK9kKdz4Ivz+i/Ds/3PX/ZufuVqMMcYkiQWU0aj5qwsk774AWcXw/m9C1WchNSvx107PhSvvgXkXw+P/BD8+G5b/0PW3GGNMElhAGYmav7p+jNoXIXsaXPotWPIZSI29im3CiEDlJ6DkTHj4s7DyY65m9P5vQkrG2JbFGDPpWUAZiS2Puzkky+6EJZ9O/pd30Xz422fgmW/AKz+E2pdcUCk7B1Jiry9kjDHxZkuvjGTple5W8KeNzy/rbU/D7/4OOhogkAHl73XNYhUXu8Bj81eMMaNkS6/E03ie/zH/ErjlDah5EapXQ/Uz8OTTbl9uKcy7yAWXueeP7/dhjEmMYDesucf198b5R7EFlIkoNQtOuNQ9AJprXHDZ/iy89Qis/aWbqT/7DFd7mXcxzFhsQ4+Nmeiqn3GDeJp2uFGhJ10R1+ytyWuyrTYcDrpJkdXPuCCzZ4NLzyiAuRe4OS5zzoOCudY8ZsxE0boLnroNNj/m1gn8wH9AxUUjzs6Wr49hUgaU/joOwPbn3GTMHX92qx4DTJnl1hibcx7MORfySpNaTGOO0NUCve2QOzvZJYmfxu0QCUNxHFcdDgfh1Z/Ac/8GGnbLP53zhVGvXG59KCa2rCI49Sr3UHV/1O8+7+bWVD8Nb650x+WXu+BS7gWYnOlJLbaZhJprYcsTsOWPbiRjJASF812/4bz3HX+jGlVh/2bYvAre/j3s3+TSZ5wGiz8OJ18JWYUjz7/2Jfjjl9015l8Kl3/H/T9OIKuhTPYaymAiEWh42wWXd//iOvp7Wt2+ohO8AHMulJ5lAcbEn6pbqXvLE26o/r6NLr34RFhwmZtMXL3a/V2GeyAl0/099gWYgjmjL0M4CO373bUCqaPPTxV2r3MBZPMqd/sKBErfA4s+7Lbf+A3seQN8KbBgmQsu89439MVk2xvg6a+6fHJL4LJvw4LL49qEbU1eMVhAGaZI2P2h1/zFBZnalw/fxyWvzAWWkjPdc/HCI5fgN2YoQj3ux8uWP8KWJ6FtN4jPfeEuuMx9MRZWHHlOb6c3qvFpN2y++V2XXjgP5l0C898HZe+NXXuJRKB9H7TUuhpQ/+eDu1xTkS8A+XOgeIF7FC1wTVNFJxx7ZYxI2K3z97ZXE2mtc4Ni5pzngsiCD0DOtCPP2bsR3ngA3nzQTQHIKoZTr3G3rph20sDXWftLWP0N95mcfTOc908JWbnDAkoMFlBGKRyE3Rug7hXY+QrUver++AHSct2aY6VnQslZMGvJ2K8kMNmpQrgXgl3u38rnd79Sxe9t+/ptx/gFG4m4Hw29fY/2Y2x3uvxSMlw7fSD98CNWWiDNPXZvcLWQ6tXQ2wYpWW6I+4IPwPz3D6/pp3G7CyzVT7tAE+p2c7LmnAuzqo4MIK11bn+07GnuB1J+mXvOmQ4Hd8OBrdCwxY2Q0vDh43NLXGCJDjaF81yN6u1V7n5G7fvAn+o6whd+2AXHzIJjv5dw0A2g2XC/C7CRYOwmsd3r4Q9fcrWf8nPhA991ZUkQCygxWECJM1X3n63u1cMBpuEdt88XgOmnutrL7Co3B8YX8L7QAt4jervvdf+0gKv6+1Lcc7yq8aqgEdcuHwkPdNAxzg+7cyOhfo/+6VHb4V73CPVEPfdAqLffc9T+UA+EulygCHa5L8Rgp5tfEOzy9nlpg5X5KOICi8/v/l3A5TXk0/2u2UnDrhzDujaQPd018Sz4gPv1Ho/+kGCXCyp9AaZph/vbiw4Y+eVRr0uPvfJFqNflc2ALNGx1f+MHtsCB6qM/r5RM1wS38MMuMI7mFhYdjbDxYRdcopvEMvJh3X2uFnPpt+CUKxM+QjMpAUVElgHfB/zAL1T1zn7704BfAUuARuAaVa3x9t0GXA+EgS+o6lOD5Skic4CVQCGwFvikqvYOVj4LKGOgs8kNU+4LMLvWHv2LcDTEHxVgAocDTV/gEV/sL/JYr8ctcb/i/WmuHT/6F39Khred6b6AUzJj7/OneAEz7L7wD21H+m2HD2+jrqaQ2vfIHnw7kHb4i0zVfabBLi8Adkc9eo5Oz5/jbiKX6GbS3s7E1ZQjEWjd6YJM4zYXnCouTsz1opvEOhvhjM/BRf8yZpOVxzygiIgf2ApcAtQDrwPXqurmqGP+HjhVVW8UkRXAR1T1GhFZBDwALAVmAs8AfWPpYuYpIg8Bj6rqShH5CfCGqv54sDJaQEmCUK/7RRfqPsav+FgBIOSaACJBbzvktsPBI/dFp2vkcICJVduJTutr/vH5gQF+4Q32y2+gmpbEqml5af7Uw4HiiOc0ty+Q5uVhc4JMDOEg9LQNrfksjpIxbHgpUK2qO7wCrASWA5ujjlkOfN3bfhj4bxERL32lqvYA74pItZcfsfIUkbeBi4CPecfc6+U7aEAxSRBIhRmnJrsUxkwM/pQxDyaDSWT9chZQF/W63kuLeYyqhoBWXJPVQOcOlF4ItHh5DHQtAETkBhFZIyJrGhoaRvC2jDHGxDLpxnWq6s9UtUpVq4qLi5NdHGOMmTASGVB2ASVRr2d7aTGPEZEAkIvrnB/o3IHSG4E8L4+BrmWMMSaBEhlQXgfmi8gcEUkFVgCr+h2zCrjO274SeFbdKIFVwAoRSfNGb80HXhsoT++c57w88PJ8LIHvzRhjTD8J65RX1ZCI3Aw8hRvie4+qbhKR24E1qroKuBu4z+t0b8IFCLzjHsJ14IeAm1TdTKJYeXqX/AqwUkS+Caz38jbGGDNGbGKjDRs2xphhGWjY8KTrlDfGGJMYFlCMMcbExaRu8hKRBqB2hKcXAQfiWJzjlX0Oh9ln4djn4Ezkz6FMVY+adzGpA8poiMiaWG2Ik419DofZZ+HY5+BMxs/BmryMMcbEhQUUY4wxcWEBZeR+luwCjBP2ORxmn4Vjn4Mz6T4H60MxxhgTF1ZDMcYYExcWUIwxxsSFBZQREJFlIrJFRKpF5NZklydZRKRGRN4SkQ0iMmnWsBGRe0Rkv4hsjEorEJGnRWSb95yfzDKOhQE+h6+LyC7vb2KDiFyezDKOBREpEZHnRGSziGwSkVu89En3N2EBZZi8Wxv/ELgMWARc692yeLK6UFUXT7Lx9r8ElvVLuxVYrarzgdXe64nulxz9OQDc5f1NLFbVx8e4TMkQAr6sqouAs4CbvO+ESfc3YQFl+A7d2lhVe4G+WxubSUJVX8Ctjh1tOe7W03jPV4xpoZJggM9h0lHVPaq6zttuA97G3TF20v1NWEAZvqHc2niyUOBPIrJWRG5IdmGSbJqq7vG29wLTklmYJLtZRN70msQmfDNPNBEpByqBV5mEfxMWUMxovFdVT8c1/90kIuclu0DjgXfDt8k6Hv/HQAWwGNgDfDe5xRk7IpINPAJ8UVUPRu+bLH8TFlCGbyi3Np4UVHWX97wf+C2uOXCy2iciMwC85/1JLk9SqOo+VQ2ragT4OZPkb0JEUnDB5H5VfdRLnnR/ExZQhm8otzae8EQkS0Ry+raB9wMbBz9rQou+nfWkvQV13xeo5yNMgr8JERHcHWLfVtXvRe2adH8TNlN+BLyhkP/J4dsQ35HkIo05EZmLq5WAu5X0bybL5yAiDwAX4JYn3wd8Dfgd8BBQirslwtWqOqE7rAf4HC7ANXcpUAN8PqofYUISkfcCfwHeAiJe8j/j+lEm19+EBRRjjDHxYE1exhhj4sICijHGmLiwgGKMMSYuLKAYY4yJCwsoxhhj4sICijHHKRG5QET+kOxyGNPHAooxxpi4sIBiTIKJyCdE5DXv/iA/FRG/iLSLyF3e/TNWi0ixd+xiEXnFW1zxt32LK4rIPBF5RkTeEJF1IlLhZZ8tIg+LyDsicr83a9uYpLCAYkwCichC4BrgHFVdDISBjwNZwBpVPQl4HjfLHOBXwFdU9VTczOu+9PuBH6rqacDZuIUXwa1s+0XcvXnmAuck/E0ZM4BAsgtgzAR3MbAEeN2rPGTgFgmMAA96x/waeFREcoE8VX3eS78X+F9vzbRZqvpbAFXtBvDye01V673XG4By4MXEvy1jjmYBxZjEEuBeVb3tiESR/9vvuJGugdQTtR3G/k+bJLImL2MSazVwpYhMhUP3GS/D/d+70jvmY8CLqtoKNIvIuV76J4HnvbsA1ovIFV4eaSKSOabvwpghsF8zxiSQqm4WkX/F3dnSBwSBm4AOYKm3bz+unwXcMuc/8QLGDuAzXvongZ+KyO1eHleN4dswZkhstWFjkkBE2lU1O9nlMCaerMnLGGNMXFgNxRhjTFxYDcUYY0xcWEAxxhgTFxZQjDHGxIUFFGOMMXFhAcUYY0xc/H/jjRBgwJsyDAAAAABJRU5ErkJggg==\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='mse')\n","score=model.evaluate(generatorTest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b62JrMaZOUhU","executionInfo":{"status":"ok","timestamp":1668921818634,"user_tz":360,"elapsed":980,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"8cc4cda4-7325-4835-f5a1-f29a25f2b7e1"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["51/51 [==============================] - 0s 3ms/step - loss: 1.4940e-04\n"]}]},{"cell_type":"code","source":["#Just load the model instead of training from sctarch\n","import tensorflow as tf\n","model_link = '/content/drive/MyDrive/IE534 itn/Project/models/timeseries models/first_two_Features.h5'\n","# model_link = '1UXPLo2YEsCtx7xDwnmoHTI3mpnp010Cq'\n","# tf.keras.models.save_model(model,model_link)\n","model=tf.keras.models.load_model(model_link)\n","#train loss\n","model.evaluate(generatorTrain)\n","\n","#test loss\n","model.evaluate(generatorTest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"2GXKNY2kdjSd","executionInfo":{"status":"ok","timestamp":1668651950990,"user_tz":360,"elapsed":4022,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"33c66edf-bb0a-483d-d5a3-c65bf09d74c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["498/498 [==============================] - 3s 5ms/step - loss: 7.1706e-04\n","51/51 [==============================] - 0s 7ms/step - loss: 1.4846e-04\n"]},{"output_type":"execute_result","data":{"text/plain":["'Difference is huge, We are overfitting :O '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["#Extrapolate to future\n","\n","predictions = []\n","#the very last window of training\n","first_batch = train[-n_input:]\n","print(\"first \",first_batch.shape)\n","current_batch = first_batch.reshape((1,n_input, n_features))\n","\n","for i in range (len(test)):\n","  # get the prediction value for the first batch\n","  #current_pred = model.predict(current_batch,verbose=0)[0]\n","  current_pred = model.predict(current_batch,verbose=0)\n","  print(current_pred.shape,\"CP\")\n","\n","  # append the prediction into the array\n","  predictions.append(current_pred)\n","\n","  # remove the first value\n","  current_batch_rmv_first = current_batch[:,1:,:]\n","\n","  print(current_batch[0].shape)\n","  print(current_batch_rmv_first[0].shape)\n","  #update the batch\n","  current_batch = np.append(current_batch_rmv_first, [[current_pred]],axis=1)\n"],"metadata":{"id":"UWu48vtjj-FL","executionInfo":{"status":"error","timestamp":1668652054749,"user_tz":360,"elapsed":423,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"colab":{"base_uri":"https://localhost:8080/","height":496},"outputId":"bbb34baa-7a75-4b35-bdc0-93f913e65e38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["first  (6, 2)\n","(1, 1) CP\n","(6, 2)\n","(5, 2)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-4c3451406a8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch_rmv_first\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m#update the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_batch_rmv_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   4815\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4816\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4817\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 4 dimension(s)"]}]},{"cell_type":"code","source":["#lets plot the actual test and prediction\n","import matplotlib.pyplot as plt\n","\n","predictions_actual_scale = scaler.inverse_transform(predictions)\n","test_data_actual_scale = scaler.inverse_transform(test)\n","\n","\n","\n","fig, axs = plt.subplots(figsize=(20, 10))\n","plt.plot(predictions_actual_scale)\n","plt.plot(test_data_actual_scale)\n","\n","\n","\n","plt.legend([\"Predictions\" , \"Actual\"])\n","plt.ylabel(\"Unemployment rate\")\n","plt.xlabel(\"Month\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"cOa_l8INlgtM","executionInfo":{"status":"error","timestamp":1668647984954,"user_tz":360,"elapsed":198,"user":{"displayName":"Hossein Mohasel Arjomandi","userId":"16009051171373450897"}},"outputId":"64a3797c-d43b-470d-b688-652e6ec6d96a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-6d7585b90860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredictions_actual_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_data_actual_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,1) doesn't match the broadcast shape (1,2)"]}]}]}